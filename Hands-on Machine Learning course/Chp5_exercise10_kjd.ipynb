{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOML Ch.5 Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "\n",
    "### Exercise: Train an SVM regressor on the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the California housing dataset and looking at a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset using scikit_learn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X = housing[\"data\"]\n",
    "y = housing[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block\\n        - HouseAge      median house age in block\\n        - AveRooms      average number of rooms\\n        - AveBedrms     average number of bedrooms\\n        - Population    block population\\n        - AveOccup      average house occupancy\\n        - Latitude      house block latitude\\n        - Longitude     house block longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttp://lib.stat.cmu.edu/datasets/\\n\\nThe target variable is the median house value for California districts.\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset and its attributes\n",
    "housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already clean, so we can jump to splitting, standardizing, and fitting our data to the linear SVR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "st_scaler = StandardScaler()\n",
    "X_train_st_scaler = st_scaler.fit_transform(X_train)\n",
    "X_test_st_scaler = st_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K\\Anaconda373\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=99, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and fit linear SVR on training data\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "lsvr = LinearSVR(random_state=99)\n",
    "lsvr.fit(X_train_st_scaler, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for training set\n",
    "y_pred_lsvr = lsvr.predict(X_train_st_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our predictions, let's find the mean squared and root mean squared errors to get a sense of how strong our model is. Both MSE and RMSE tell us how well our model fits  the data. The lower the values, the better. The advantage of RMSE is that it has the same units as the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.9235205896401963\n",
      "RMSE: 0.9609997864933146\n"
     ]
    }
   ],
   "source": [
    "# Find the mean squared and root mean squared errors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_lsvr = mean_squared_error(y_train, y_pred_lsvr)\n",
    "rmse_lsvr = np.sqrt(mse_lsvr)\n",
    "print('MSE: ' + str(mse_lsvr))\n",
    "print('RMSE: ' +str(rmse_lsvr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the RMSE error values are in the same units as the target value, where the target values are in the tens of thousands of dollars, it tells us that our predictions may be off by about $10,000, which isn't great. We'll try another SVR model, this time using an RBF kernel to see if we can decrease our error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and fit SVR on default 'rbf' kernal value\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train_st_scaler, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set predictions\n",
    "y_pred_svr = svr.predict(X_train_st_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3438182495435124\n",
      "RMSE: 0.5863601704955005\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE and RSME\n",
    "mse_svr = mean_squared_error(y_train, y_pred_svr)\n",
    "rmse_svr = np.sqrt(mse_svr)\n",
    "print('MSE: ' + str(mse_svr))\n",
    "print('RMSE: ' +str(rmse_svr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE and RMSE values for SVR with RBF are considerably lower than those of linear SVR, with our error reduced to around $6,000. So, let's tune the RBF SVR model \n",
    "to see if we can lower the error values even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] C=4, gamma=0.01 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ C=4, gamma=0.01, score=0.6756234654033157, total=  22.6s\n",
      "[CV] C=4, gamma=0.01 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ C=4, gamma=0.01, score=0.6699661097878262, total=  22.1s\n",
      "[CV] C=4, gamma=0.01 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ C=4, gamma=0.01, score=0.6803028800440785, total=  23.6s\n",
      "[CV] C=4, gamma=0.1 ..................................................\n",
      "[CV] ......... C=4, gamma=0.1, score=0.7397280347318287, total=  24.8s\n",
      "[CV] C=4, gamma=0.1 ..................................................\n",
      "[CV] ......... C=4, gamma=0.1, score=0.7389211810033438, total=  25.8s\n",
      "[CV] C=4, gamma=0.1 ..................................................\n",
      "[CV] ......... C=4, gamma=0.1, score=0.7395810433063787, total=  25.4s\n",
      "[CV] C=4, gamma=1 ....................................................\n",
      "[CV] ............ C=4, gamma=1, score=0.749676258724927, total=  47.2s\n",
      "[CV] C=4, gamma=1 ....................................................\n",
      "[CV] ............ C=4, gamma=1, score=0.755795056412663, total=  46.6s\n",
      "[CV] C=4, gamma=1 ....................................................\n",
      "[CV] ........... C=4, gamma=1, score=0.7468779028206765, total=  45.1s\n",
      "[CV] C=6, gamma=0.01 .................................................\n",
      "[CV] ........ C=6, gamma=0.01, score=0.6795553049728964, total=  23.2s\n",
      "[CV] C=6, gamma=0.01 .................................................\n",
      "[CV] ........ C=6, gamma=0.01, score=0.6756598017099387, total=  21.8s\n",
      "[CV] C=6, gamma=0.01 .................................................\n",
      "[CV] ........ C=6, gamma=0.01, score=0.6853963527334551, total=  21.5s\n",
      "[CV] C=6, gamma=0.1 ..................................................\n",
      "[CV] ......... C=6, gamma=0.1, score=0.7446847542231214, total=  25.8s\n",
      "[CV] C=6, gamma=0.1 ..................................................\n",
      "[CV] ......... C=6, gamma=0.1, score=0.7428476292097137, total=  26.1s\n",
      "[CV] C=6, gamma=0.1 ..................................................\n",
      "[CV] ......... C=6, gamma=0.1, score=0.7425692917007395, total=  25.7s\n",
      "[CV] C=6, gamma=1 ....................................................\n",
      "[CV] ........... C=6, gamma=1, score=0.7461937209525235, total=  58.6s\n",
      "[CV] C=6, gamma=1 ....................................................\n",
      "[CV] ........... C=6, gamma=1, score=0.7542604696345538, total=  56.5s\n",
      "[CV] C=6, gamma=1 ....................................................\n",
      "[CV] ........... C=6, gamma=1, score=0.7465109788195862, total=  58.4s\n",
      "[CV] C=8, gamma=0.01 .................................................\n",
      "[CV] ........ C=8, gamma=0.01, score=0.6825251561270919, total=  22.7s\n",
      "[CV] C=8, gamma=0.01 .................................................\n",
      "[CV] ........ C=8, gamma=0.01, score=0.6792232181748327, total=  22.0s\n",
      "[CV] C=8, gamma=0.01 .................................................\n",
      "[CV] ........ C=8, gamma=0.01, score=0.6884837149922356, total=  23.2s\n",
      "[CV] C=8, gamma=0.1 ..................................................\n",
      "[CV] ......... C=8, gamma=0.1, score=0.7476595372125503, total=  30.6s\n",
      "[CV] C=8, gamma=0.1 ..................................................\n",
      "[CV] ......... C=8, gamma=0.1, score=0.7448204747813063, total=  32.7s\n",
      "[CV] C=8, gamma=0.1 ..................................................\n",
      "[CV] ............ C=8, gamma=0.1, score=0.7445478722031, total=  32.4s\n",
      "[CV] C=8, gamma=1 ....................................................\n",
      "[CV] ........... C=8, gamma=1, score=0.7431869758655509, total= 1.2min\n",
      "[CV] C=8, gamma=1 ....................................................\n",
      "[CV] ........... C=8, gamma=1, score=0.7520942373506266, total= 1.3min\n",
      "[CV] C=8, gamma=1 ....................................................\n",
      "[CV] ........... C=8, gamma=1, score=0.7449297941806179, total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed: 20.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [4, 6, 8], 'gamma': [0.01, 0.1, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import GridsearchCV and tune based on gamma and C\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'C': [4, 6, 8], 'gamma': [0.01, 0.1, 1]}\n",
    "\n",
    "svr = SVR()\n",
    "grid_search = GridSearchCV(svr, params, cv=3, verbose=3)\n",
    "grid_search.fit(X_train_st_scaler, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 4, 'gamma': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameter values from grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=4, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=1,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the tuned model\n",
    "svr_params = SVR(C=4, gamma=1)\n",
    "svr_params.fit(X_train_st_scaler, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2032781729709123\n",
      "RMSE: 0.4508638075637834\n"
     ]
    }
   ],
   "source": [
    "# MSE and RMSE values of the tuned model\n",
    "y_pred_svr_params = svr_params.predict(X_train_st_scaler)\n",
    "mse_svr_params = mean_squared_error(y_train, y_pred_svr_params)\n",
    "rmse_svr_params = np.sqrt(mse_svr_params)\n",
    "print('MSE: ' + str(mse_svr_params))\n",
    "print('RMSE: ' +str(rmse_svr_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2862598968262924\n",
      "RMSE: 0.5350326128623305\n"
     ]
    }
   ],
   "source": [
    "# Run the tuned model on the test data\n",
    "y_pred_svr_params = svr_params.predict(X_test_st_scaler)\n",
    "mse_svr_params = mean_squared_error(y_test, y_pred_svr_params)\n",
    "rmse_svr_params = np.sqrt(mse_svr_params)\n",
    "print('MSE: ' + str(mse_svr_params))\n",
    "print('RMSE: ' +str(rmse_svr_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tuned model did considerably better than our original RBF model, reducing our error to around $4,500. However, the error values on test data are a bit higher than on the training data, suggesting some degree of overfitting. If we wished to, we could continue tuning the model further to reduce overfitting, but we'll stop here since we've met the requirements of this particular exercise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
