{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RATM Music Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf94tM0tXbL7"
      },
      "source": [
        "# RATM Music Generator\n",
        "\n",
        "The purpose of this project is to develop a recurrent neural network (RNN) model that generates music, specifically, it generates music after having been trained on popular 1990's band Rage Against the Machine (RATM). \n",
        "\n",
        "Why RATM? Partly because it's a bit different than the classical music that's typical of similar projects, and partly because we don't have an ear for classical music.     \n",
        "\n",
        "This project is based in large part on a very similar project by Sigurður Skúli in which he designs a model that learns to create piano based music. Much of the code that we use is based on his work. His project can be found here: https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5\n",
        "\n",
        "A couple of other projects also borrow Skúli's code and put their own twist on the model. Those can be found here: https://becominghuman.ai/generating-music-using-lstm-neural-network-545f3ac57552\n",
        "and here: https://medium.com/@leesurkis/how-to-generate-techno-music-using-deep-learning-17c06910e1b3\n",
        "\n",
        "One significant change that we're going to be making to our model compared to the aforementioned models is that we'll be using GRU layers rather than LSTM layers. A GRU layer is a simplified version of an LSTM layer but often gives similar results. Our hope is that this will reduce the training time required since some of the other projects mentioned took upwards of 18 hours of training time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyuhWY1Qcgnu"
      },
      "source": [
        "We'll start by downloading several libraries and layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgRWqhCyfQSj"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy7H50CycBO5"
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "from music21 import converter, instrument, midi, note, stream, chord\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b6LOdOsfLJZ"
      },
      "source": [
        "# Random seeds from both numpy and tensorflow\n",
        "from numpy.random import seed\n",
        "seed(99)\n",
        "tf.random.set_seed(99)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18_DGqsZcpkN"
      },
      "source": [
        "Our music files are in MIDI (Musical Instrument Digital Interface) format, which allows for small files that contain information related to notes, tempo, instruments, duration, and pitch but without the sound. These patterns can then be run through software such as Logic, Garageband, or Audacity to generate music.\n",
        "\n",
        "The original MIDI files are located at this site: https://freemidi.org/artist-826-rage-against-the-machine-P-0\n",
        "\n",
        "Since we're running this on Colab, we've conveniently stored the files on Google Drive, and we'll access them from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4lZtE8MfLRY"
      },
      "source": [
        "# Mount Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsZsGTThfo76"
      },
      "source": [
        "For the following two functions, more information can be found on Skúli's post. Essentially, what they'll allow us to do is extract notes form our MIDI files.  To do this, we're going to be using a Python toolkit called Music21. More details, including documentation can be found here: https://web.mit.edu/music21/doc/about/what.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Mxcd7LcNAA"
      },
      "source": [
        "def train_network():\n",
        "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
        "    notes = get_notes()\n",
        "\n",
        "    # get amount of pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "    model = create_network(network_input, n_vocab)\n",
        "\n",
        "    train(model, network_input, network_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MGwk2x_cNCw"
      },
      "source": [
        "def get_notes():\n",
        "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
        "    notes = []\n",
        "\n",
        "    for file in glob.glob(\"/content/drive/MyDrive/ratm midi/*.mid\"):\n",
        "        midi = converter.parse(file)\n",
        "\n",
        "        print(\"Parsing %s\" % file)\n",
        "\n",
        "        notes_to_parse = None\n",
        "\n",
        "        try: # file has instrument parts\n",
        "            s2 = instrument.partitionByInstrument(midi)\n",
        "            notes_to_parse = s2.parts[0].recurse() \n",
        "        except: # file has notes in a flat structure\n",
        "            notes_to_parse = midi.flat.notes\n",
        "\n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                notes.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    with open('/content/drive/MyDrive/ratm midi/data/notes', 'wb') as filepath:\n",
        "        pickle.dump(notes, filepath)\n",
        "\n",
        "    return notes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diAOAZMzf-e2"
      },
      "source": [
        "Listening to the songs, we notice that there are more instruments present in every song than in the piano music that Skúli used. That's going to make our task potentially more difficult because we'll need to extract the notes of multiple instruments. \n",
        "\n",
        "We'll need to take a look at which instruments are present in each song. We can use some code referenced here to do so: https://www.kaggle.com/wfaria/midi-music-data-extraction-using-music21"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTI5_0SaBP-Z",
        "outputId": "90f16bbb-1335-429d-fa9d-e9db970f3557"
      },
      "source": [
        "# List of instruments per song\n",
        "for file in glob.glob(\"/content/drive/MyDrive/ratm midi/*.mid\"):\n",
        "    midi = converter.parse(file)\n",
        "    s2 = instrument.partitionByInstrument(midi) \n",
        "    print(\"List of instruments found on:\" + str(file)) \n",
        "    partStream = s2.parts.stream()\n",
        "    for p in partStream:\n",
        "        aux = p\n",
        "        print (p.partName)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of instruments found on:/content/drive/MyDrive/ratm midi/AshesInTheFall.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/Bombtrack.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/BornOfABrokenMan.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/BornAsGhosts.mid\n",
            "None\n",
            "Acoustic Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/BulletInTheHead.mid\n",
            "None\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/BullsOnParade.mid\n",
            "None\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/CalmLikeABomb.mid\n",
            "None\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/DownOnTheStreet.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/DownRodeo.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/FistfulOfSteel.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/Freedom.mid\n",
            "None\n",
            "Celesta\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/FuckThePolice.mid\n",
            "None\n",
            "Electric Bass\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/GuerillaRadio.mid\n",
            "None\n",
            "Electric Bass\n",
            "Contrabass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/HowICouldJustKillAMan.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/ImHousin.mid\n",
            "Electric Bass\n",
            "None\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/InMyEyes.mid\n",
            "Electric Bass\n",
            "None\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/KickOuttheJams.mid\n",
            "None\n",
            "Electric Bass\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/KillingInTheNameOf.mid\n",
            "Electric Organ\n",
            "None\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/KnowYourEnemy.mid\n",
            "None\n",
            "Electric Organ\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/MaggiesFarm.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/Maria.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/MicCheck.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/MicrophoneFiend.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/NewMillenniumHomes.mid\n",
            "None\n",
            "Electric Bass\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/NoShelter.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/PeopleOfTheSun.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/PistolGripPump.mid\n",
            "None\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/RenegadesofFunk.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/Revolver.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/RollRight.mid\n",
            "None\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/SettleforNothing.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/SleepNowInTheFire.mid\n",
            "None\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/Snakecharmer.mid\n",
            "None\n",
            "Electric Bass\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/StreetFightingMan.mid\n",
            "None\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/TakeThePowerBack.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "Electric Guitar\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/Testify.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/TheGhostOfTomJoad.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/TireMe.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/TownshipRebellion.mid\n",
            "None\n",
            "Electric Guitar\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/Vietnow.mid\n",
            "None\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/VoiceOfTheVoiceless.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/WakeUp.mid\n",
            "None\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/WarWithinABreath.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/WindBelow.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/WithoutAFace.mid\n",
            "None\n",
            "Electric Bass\n",
            "List of instruments found on:/content/drive/MyDrive/ratm midi/YearOfThaBoomerang.mid\n",
            "None\n",
            "Electric Bass\n",
            "Piano\n",
            "Electric Guitar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBggBxilhOX1"
      },
      "source": [
        "Looking through this list, piano and electric bass are present in just about every song. Interestingly, RATM generally doesn't use piano in their music. The three primary instruments that they use are electric guitar, electric bass, and drums. Drums aren't listed for any of the songs and electric guitar is only mentioned for a couple. There's also a value of 'None' listed for most of the songs. Unfortunately, what that means is that there are unidentified instrument notes that we won't be able to extract. As our result, the music that we eventually generate won't be as rich and complex. \n",
        "\n",
        "Next, we're going to prepare the notes to be fed into our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3rbeoYzcNFI"
      },
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    # get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "     # create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    network_input = network_input / float(n_vocab)\n",
        "\n",
        "    network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ku3fmNhAniC"
      },
      "source": [
        "We're going to use a an RNN structure similar to that used by Skúli (if we choose to revisit this project in the future, we'll likely try other model configurations). However, we are making a couple of changes. First, we're going to use GRU layers rather than LSTM and we're not going to keep the dropout layers (but we've left them in the code as comments in case others may want to running the network with the dropout layers). The main reason for these changes is to speed up the learning process. While this may lead to music that doesn't sound as interesting as that generated by Skúli, the training should be quite a bit faster. Hopefully, the trade off is worth it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AKD5HIcNHu"
      },
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(GRU(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(GRU(512, return_sequences=True))\n",
        "    model.add(GRU(512))\n",
        "    model.add(BatchNorm())\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5-S9RihB9Tl"
      },
      "source": [
        "It's expected that the more epochs that we let the model run, the better the results. However, we're also going to use ModelCheckpoint so that we can end the process early if we see the loss decrease stalling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj7_bGhQcNOu"
      },
      "source": [
        "def train(model, network_input, network_output):\n",
        "    \"\"\" train the neural network \"\"\"\n",
        "    filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor='loss',\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    model.fit(network_input, network_output, epochs=100, batch_size=128, callbacks=callbacks_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_network()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTGCaw49Cy2S"
      },
      "source": [
        "We ran the model for about 70 epochs before the loss decrease stalled and actually began increasing. \n",
        "\n",
        "Now, let's move to the second half of this project - generating music. We need to define functions that load the notes that were used to train the model and prepare the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ler5Qd-mcNUd"
      },
      "source": [
        "def generate():\n",
        "    \"\"\" Generate a midi file \"\"\"\n",
        "    #load the notes used to train the model\n",
        "    with open('/content/drive/MyDrive/ratm midi/data/notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    # Get all pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
        "    model = create_network(normalized_input, n_vocab)\n",
        "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
        "    create_midi(prediction_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fJeKvqPc8oh"
      },
      "source": [
        "def prepare_sequences(notes, pitchnames, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    # map between notes and integers and back\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    sequence_length = 100\n",
        "    network_input = []\n",
        "    output = []\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM/GRU layers\n",
        "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    normalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "    return (network_input, normalized_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X8ghgRK90vv"
      },
      "source": [
        "We'll now load out best weights. To do this, we'll need to go to our Drive where the weights are saved and copy the filepath."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPaijfD9c8rr"
      },
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(GRU(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(GRU(512, return_sequences=True))\n",
        "    model.add(GRU(512))\n",
        "    model.add(BatchNorm())\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    # Load the weights to each node (copy filepath from Drive)\n",
        "    model.load_weights('/content/weights-improvement-51-0.0653-bigger.hdf5')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjwqFnOBc8uj"
      },
      "source": [
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP4_g7d2-QyN"
      },
      "source": [
        "For our model, piano and electric bass were the two most common instruments, having been found in almost every song. So, we'll generate 500 notes for each instrument, which comes out to about two minutes of music. We'll generate music for both instruments separately and combine them into a single song using Audacity (you can use any music editing software that allows song layering). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNe9nVrtc8xg"
      },
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp='/content/drive/MyDrive/ratm midi/data/test_output1.mid')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    generate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Cp0Mc7c80n"
      },
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.ElectricBass()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.ElectricBass()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp='/content/drive/MyDrive/ratm midi/data/test_output2.mid')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    generate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Kb9RVD_GBR"
      },
      "source": [
        "You can find the result of the best combination of instruments after multiple attempts in the same GitHub folder as this notebook.\n",
        "\n",
        "It certainly doesn't sound like Rage Against the Machine, but the result is understandable given that we weren't able to account for electric guitar and drums. In addition, we also trained the model fairly rapidly, using GRU rather than LSTM and not implementing dropout, which may have decreased the quality of the music. But it was an attempt worth trying simply to see how well faster training methods can compare and how much information is actually lost. In our case, the training time was about a quarter of that mentioned in the other projects. If, in the future, we can generate music for the missing instruments, we may be able to create songs that sound more complex and robust and may allow us to claim that the trade off between speed and quality is worth it.   "
      ]
    }
  ]
}